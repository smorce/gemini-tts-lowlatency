# Gemini TTS ストリーミング再生アプリ

> [!NOTE]
> このプロジェクトは[vibe coding](https://ja.wikipedia.org/wiki/%E3%83%90%E3%82%A4%E3%83%96%E3%82%B3%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0)で作成しました。

使用するときは index.html の以下を修正してください。
```
window.GEMINI_KEY = '********************************************';
```

次にルートディレクトリに移動して
```
python -m http.server 8000
```
でサーバーを起動し、<br>
http://localhost:8000/<br>
にアクセスしてください。

<br>
<br>

https://ai.google.dev/gemini-api/docs/speech-generation?hl=ja

<br>
<br>


最初に押さえておきたいポイントを一文でまとめると――
**再生ボタンを押してから音が出るまで数百 ms〜1 秒ほど待たされるのは、TTS サーバ側で最初の PCM チャンクを生成・エンコードし、ネットワークとブラウザがそれを受け取ってオーディオパイプラインを起動・バッファリングするまでに物理的な時間がかかるためで、ストリーミング方式そのものが失敗しているわけではありません**。

---

## なぜクリック直後に無音の間が生じるのか

### 1. サーバ側での初期処理

* **テキスト前処理 & トークナイズ** – いきなり音が作られるわけではなく、まず入力文字列が内部表現に変換されます。
* **最初の推論パス** – `gemini‑2.5‑flash-preview-tts` は 16‑bit PCM を 128 frame 単位（約 5.3 ms\@24 kHz）で生成しますが、最初の 1〜2 block をまとめて出力してからストリームに流します。公開ベンチでは **150–400 ms** が典型値と報告されています。

### 2. ネットワーク往復 (RTT)

* TLS ハンドシェイクと HTTP/2 送信に 50–150 ms。遠距離リージョンを叩くとここが跳ね上がります。

### 3. ブラウザでの復号・パイプライン起動

* 受信した base‑64 PCM を **デコード → Int16 → Float32** に変換しつつ **AudioWorklet** にポスト。ここで 1〜2 ms。
* AudioWorklet は 128 frame ごとに `process()` が走るため、最低 128 frame（2.67 ms\@48 kHz） バッファが埋まるまで再生を開始しません。
* ユーザー操作で一度 suspend された `AudioContext` を `resume()` するコストが 20‑50 ms 程度。

結果として **合計 0.3〜1.0 秒程度の“初動レイテンシー”** が生じるのが実測上の多くの環境での挙動です。

---

## 「ストリーミング＝即時再生」ではない理由

| 段階                | ボトルネック           | 実時間の目安                  |
| ----------------- | ---------------- | ----------------------- |
| テキスト → 第1PCM      | モデル推論            | 150–400 ms |
| ネットワーク往復          | TLS + HTTP/2     | 50–150 ms               |
| ブラウザセットアップ        | base‑64 decode 等 | 20–100 ms               |
| AudioWorklet バッファ | 128‑256 frame    | 3–6 ms     |

Streaming とは「**次のチャンクが届く前に再生が始まる**」方式であり、クリックと同時に音が出るわけではありません。Live API が <150 ms を目指すのは、この初動を専用双方向チャネルで最適化しているためです。

---

## 待ち時間をさらに縮める 6 つの実践策

### 1. 接続とモデルを“温めて”おく

```js
await ai.models.generateContent({ /* 空 or 1文字 */ });
```

最初のリクエストでモデルをメモリにロードし、HTTP/2 コネクションも張ったままにすることで次回の初動が 100 ms 以上短縮される例があります。

### 2. テキストを短いフラグメントに区切る

長文を一度に渡すとモデルが全文を解析してから音を出すため遅くなります。**文単位で分割して逐次 `generateContentStream()`** すると先頭文の再生が早まります。

### 3. 音声速度 (`speed`) を 1.05〜1.20 倍に

生成が物理的に速くなるわけではありませんが、**再生が短くなる＝バッファ消費が速くなる** ため次文への切り替えがスムーズに

### 4. 近いリージョンを指定

`genai.Client({apiEndpoint: "us-central1-aiplatform.googleapis.com"})` など、ユーザーと同じ大陸のリージョンに固定すると RTT を 30–60 ms 程度削減可能。

### 5. AudioContext を事前に `resume()`

ユーザー操作のたびに Context を作るのではなく、ページ初期化時に作って suspend しておくと再開コストが小さくなります。

### 6. バッファ長を自動調整するリングバッファ

ポインタがぶつかったら書き込みを止める現行実装は安全ですが、**残量が少なくなったら Worklet 内で動的に 256 → 512 frame に拡張** するなどの適応制御でクリック音を減らしつつ待ち時間も圧縮できます。

---

## 期待できる“実用値”と限界

| パイプライン                                      | 体感レイテンシー                                             |
| ------------------------------------------- | ---------------------------------------------------- |
| 現行 (`generateContentStream` + AudioWorklet) | 300–900 ms (大多数のケース)                    |
| 上記チューニング適用                                  | 180–450 ms                                           |
| Live API（双方向）                               | 80–200 ms (Google 公称) |

モデル側の計算がボトルネックのため **クリック→0 ms 再生** は難しいものの、ネットワークとクライアント側を最適化すれば **0.2–0.4 秒台** で「すぐ鳴る」体験には到達できます。

---

## まとめ

* 初動遅延は **モデル推論 + ネットワーク + ブラウザ初期化** が積み重なった必然的なコスト。
* Streaming でも「一瞬待ってから鳴り続ける」が正常挙動。
* **接続のウォームアップ・短文分割・リージョン最適化・AudioContext 事前準備** などで 100–300 ms は短縮可能。
* さらなるリアルタイム性が必要なら Live API も検討。

これで疑問点が解消されるはずです。ぜひ上記の調整を順番に試して、ターゲットとするレイテンシーに近づけてみてください。

## ライセンス

本ソフトウェアは Unlicense に準拠し、パブリックドメインとして公開されています。
